@article{deerwester1990indexing,
  title={Indexing by latent semantic analysis},
  author={Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Harshman, Richard},
  journal={Journal of the American society for information science},
  volume={41},
  number={6},
  pages={391},
  year={1990},
  publisher={American Documentation Institute}
}

@inproceedings{hofmann1999probabilistic,
  title={Probabilistic latent semantic analysis},
  author={Hofmann, Thomas},
  booktitle={Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence},
  pages={289--296},
  year={1999},
  organization={Morgan Kaufmann Publishers Inc.}
}

@article{blei2003latent,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of machine Learning research},
  volume=3,
  number={Jan},
  pages={993--1022},
  year=2003
}

@article{srivastava-2017-autoen-variat,
  author =       {Srivastava, Akash and Sutton, Charles},
  title =        {Autoencoding Variational Inference For Topic Models},
  journal =      {CoRR},
  year =         2017,
  url =          {http://arxiv.org/abs/1703.01488v1},
  abstract =     {Topic models are one of the most popular methods for
                  learning representations of text, but a major
                  challenge is that any change to the topic model
                  requires mathematically deriving a new inference
                  algorithm. A promising approach to address this
                  problem is autoencoding variational Bayes (AEVB),
                  but it has proven diffi- cult to apply to topic
                  models in practice. We present what is to our
                  knowledge the first effective AEVB based inference
                  method for latent Dirichlet allocation (LDA), which
                  we call Autoencoded Variational Inference For Topic
                  Model (AVITM). This model tackles the problems
                  caused for AEVB by the Dirichlet prior and by
                  component collapsing. We find that AVITM matches
                  traditional methods in accuracy with much better
                  inference time. Indeed, because of the inference
                  network, we find that it is unnecessary to pay the
                  computational cost of running variational
                  optimization on test data. Because AVITM is black
                  box, it is readily applied to new topic models. As a
                  dramatic illustration of this, we present a new
                  topic model called ProdLDA, that replaces the
                  mixture model in LDA with a product of experts. By
                  changing only one line of code from LDA, we find
                  that ProdLDA yields much more interpretable topics,
                  even if LDA is trained via collapsed Gibbs
                  sampling.},
  archivePrefix ={arXiv},
  eprint =       {1703.01488},
  primaryClass = {stat.ML},
}

@article{dumoulin-2016-adver-learn-infer,
  author =       {Dumoulin, Vincent and Belghazi, Ishmael and Poole,
                  Ben and Mastropietro, Olivier and Lamb, Alex and
                  Arjovsky, Martin and Courville, Aaron},
  title =        {Adversarially Learned Inference},
  journal =      {CoRR},
  year =         2016,
  url =          {http://arxiv.org/abs/1606.00704v3},
  abstract =     {We introduce the adversarially learned inference
                  (ALI) model, which jointly learns a generation
                  network and an inference network using an
                  adversarial process. The generation network maps
                  samples from stochastic latent variables to the data
                  space while the inference network maps training
                  examples in data space to the space of latent
                  variables. An adversarial game is cast between these
                  two networks and a discriminative network is trained
                  to distinguish between joint latent/data-space
                  samples from the generative network and joint
                  samples from the inference network. We illustrate
                  the ability of the model to learn mutually coherent
                  inference and generation networks through the
                  inspections of model samples and reconstructions and
                  confirm the usefulness of the learned
                  representations by obtaining a performance
                  competitive with state-of-the-art on the
                  semi-supervised SVHN and CIFAR10 tasks.},
  archivePrefix ={arXiv},
  eprint =       {1606.00704},
  primaryClass = {stat.ML},
}

@inproceedings{blei2005correlated,
  title={Correlated topic models},
  author={Blei, David M and Lafferty, John D},
  booktitle={Proceedings of the 18th International Conference on Neural Information Processing Systems},
  pages={147--154},
  year={2005},
  organization={MIT Press}
}


@inproceedings{blei2006dynamic,
  title={Dynamic topic models},
  author={Blei, David M and Lafferty, John D},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={113--120},
  year={2006},
  organization={ACM}
}

@article{wang-2012-contin-time,
  author =       {Wang, Chong and Blei, David and Heckerman, David},
  title =        {Continuous Time Dynamic Topic Models},
  journal =      {CoRR},
  year =         2012,
  url =          {http://arxiv.org/abs/1206.3298v2},
  abstract =     {In this paper, we develop the continuous time
                  dynamic topic model (cDTM). The cDTM is a dynamic
                  topic model that uses Brownian motion to model the
                  latent topics through a sequential collection of
                  documents, where a "topic" is a pattern of word use
                  that we expect to evolve over the course of the
                  collection. We derive an efficient variational
                  approximate inference algorithm that takes advantage
                  of the sparsity of observations in text, a property
                  that lets us easily handle many time points. In
                  contrast to the cDTM, the original discrete-time
                  dynamic topic model (dDTM) requires that time be
                  discretized. Moreover, the complexity of variational
                  inference for the dDTM grows quickly as time
                  granularity increases, a drawback which limits
                  fine-grained discretization. We demonstrate the cDTM
                  on two news corpora, reporting both predictive
                  perplexity and the novel task of time stamp
                  prediction.},
  archivePrefix ={arXiv},
  eprint =       {1206.3298},
  primaryClass = {cs.IR},
}

@inproceedings{mei2007automatic,
  title={Automatic labeling of multinomial topic models},
  author={Mei, Qiaozhu and Shen, Xuehua and Zhai, ChengXiang},
  booktitle={Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={490--499},
  year=2007,
  organization={ACM}
}

@inproceedings{lau2011automatic,
  title={Automatic labelling of topic models},
  author={Lau, Jey Han and Grieser, Karl and Newman, David and Baldwin, Timothy},
  booktitle={Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1},
  pages={1536--1545},
  year={2011},
  organization={Association for Computational Linguistics}
}

@article{griffiths2002gibbs,
  title={Gibbs sampling in the generative model of latent dirichlet allocation},
  author={Griffiths, Tom},
  year={2002},
  journal={Stanford University},
  publisher={Citeseer}
}

@article{explainingthegibbssampler,
 ISSN = {00031305},
 URL = {http://www.jstor.org/stable/2685208},
 abstract = {Computer-intensive algorithms, such as the Gibbs sampler, have become increasingly popular statistical tools, both in applied and theoretical work. The properties of such algorithms, however, may sometimes not be obvious. Here we give a simple explanation of how and why the Gibbs sampler works. We analytically establish its properties in a simple case and provide insight for more complicated cases. There are also a number of examples.},
 author = {George Casella and Edward I. George},
 journal = {The American Statistician},
 number = {3},
 pages = {167-174},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Explaining the Gibbs Sampler},
 volume = {46},
 year = {1992}
}

@article{asuncion-2012-smoot-infer,
  author =       {Asuncion, Arthur and Welling, Max and Smyth,
                  Padhraic and Teh, Yee Whye},
  title =        {On Smoothing and Inference for Topic Models},
  journal =      {CoRR},
  year =         2012,
  url =          {http://arxiv.org/abs/1205.2662v1},
  abstract =     {Latent Dirichlet analysis, or topic modeling, is a
                  flexible latent variable framework for modeling
                  high-dimensional sparse count data. Various learning
                  algorithms have been developed in recent years,
                  including collapsed Gibbs sampling, variational
                  inference, and maximum a posteriori estimation, and
                  this variety motivates the need for careful
                  empirical comparisons. In this paper, we highlight
                  the close connections between these approaches. We
                  find that the main differences are attributable to
                  the amount of smoothing applied to the counts. When
                  the hyperparameters are optimized, the differences
                  in performance among the algorithms diminish
                  significantly. The ability of these algorithms to
                  achieve solutions of comparable accuracy gives us
                  the freedom to select computationally efficient
                  approaches. Using the insights gained from this
                  comparative study, we show how accurate topic models
                  can be learned in several seconds on text corpora
                  with thousands of documents.},
  archivePrefix ={arXiv},
  eprint =       {1205.2662},
  primaryClass = {cs.LG},
}

@incollection{teh2011dirichlet,
  title={Dirichlet process},
  author={Teh, Yee Whye},
  booktitle={Encyclopedia of machine learning},
  pages={280--287},
  year={2011},
  publisher={Springer}
}

@inproceedings{wallach2009evaluation,
  title={Evaluation methods for topic models},
  author={Wallach, Hanna M and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={1105--1112},
  year={2009},
  organization={ACM}
}

@article{blei2010nested,
  title={The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies},
  author={Blei, David M and Griffiths, Thomas L and Jordan, Michael I},
  journal={Journal of the ACM (JACM)},
  volume={57},
  number={2},
  pages={7},
  year={2010},
  publisher={ACM}
}

@article{gershman2012tutorial,
  title={A tutorial on Bayesian nonparametric models},
  author={Gershman, Samuel J and Blei, David M},
  journal={Journal of Mathematical Psychology},
  volume={56},
  number={1},
  pages={1--12},
  year={2012},
  publisher={Elsevier}
}

@inproceedings{gopalan2014bayesian,
  title={Bayesian nonparametric poisson factorization for recommendation systems},
  author={Gopalan, Prem and Ruiz, Francisco J and Ranganath, Rajesh and Blei, David},
  booktitle={Artificial Intelligence and Statistics},
  pages={275--283},
  year={2014}
}

@inproceedings{hinton2009replicated,
  title={Replicated softmax: an undirected topic model},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  booktitle={Advances in neural information processing systems},
  pages={1607--1614},
  year={2009}
}

@inproceedings{larochelle2012neural,
  title={A neural autoregressive topic model},
  author={Larochelle, Hugo and Lauly, Stanislas},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2708--2716},
  year={2012}
}

@inproceedings{rosen2004author,
  title={The author-topic model for authors and documents},
  author={Rosen-Zvi, Michal and Griffiths, Thomas and Steyvers, Mark and Smyth, Padhraic},
  booktitle={Proceedings of the 20th conference on Uncertainty in artificial intelligence},
  pages={487--494},
  year={2004},
  organization={AUAI Press}
}

@inproceedings{cao2015novel,
  title={A Novel Neural Topic Model and Its Supervised Extension.},
  author={Cao, Ziqiang and Li, Sujian and Liu, Yang and Li, Wenjie and Ji, Heng},
  booktitle={AAAI},
  pages={2210--2216},
  year={2015}
}

@article{chib1995understanding,
  title={Understanding the metropolis-hastings algorithm},
  author={Chib, Siddhartha and Greenberg, Edward},
  journal={The american statistician},
  volume={49},
  number={4},
  pages={327--335},
  year={1995},
  publisher={Taylor \& Francis Group}
}

@article{griffiths2004finding,
  title={Finding scientific topics},
  author={Griffiths, Thomas L and Steyvers, Mark},
  journal={Proceedings of the National academy of Sciences},
  volume={101},
  number={suppl 1},
  pages={5228--5235},
  year={2004},
  publisher={National Acad Sciences}
}

@inproceedings{li2014reducing,
  title={Reducing the sampling complexity of topic models},
  author={Li, Aaron Q and Ahmed, Amr and Ravi, Sujith and Smola, Alexander J},
  booktitle={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={891--900},
  year={2014},
  organization={ACM}
}
