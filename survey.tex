\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{lmodern}
\usepackage{courier}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\graphicspath{ {figures/} }


\lstset{basicstyle=\ttfamily\footnotesize,breaklines=true}

\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
  /Title (Topic Modeling for Scientific Documents)
  /Author (Jethro Kuan)}
\begin{document}
\nocopyright
\bibliographystyle{plain}

\title{Topic Modeling for Scientific Documents}
\author{Jethro Kuan \\
  WING-NUS\\
}
\maketitle
\begin{abstract}
  \begin{quote}
    Topic models are statistical models, used to discover abstract
    topics that occur in a collection of documents. In this paper, we
    focus on the technique of topic modeling, particularly in its
    application to the domain of scientific documents. We discuss
    different approaches to topic modeling, and weigh in on their
    strengths and shortcomings.
  \end{quote}
\end{abstract}

\section{Introduction}
In this information age, the availability of knowledge is
insufficiently met with the tools to navigate it. Tools like Semantic
Scholar use machine learning to process scientific documents and
extract meaningful structure, empowering researchers to discover
papers more relevant to their work. One such technique is topic
modeling.

Topic modeling attempts to discover abstract topics within documents.
Most topic models are generative models, assuming that latent
variables govern the generative process of a document. A document is
often modeled to be produced from a distribution of topics, and topics
themselves are distributions over the vocabulary of the corpora.

A topic model trained on a corpora of scientific documents could learn
topics such as ``Neural Networks'', ``Biology'' and ``Medicine''.
Topics attribute a high probability to words that relate to the topic.
For example, a ``Neural Networks'' topic would give attribute a high
probability to ``classifiers'', and a low probability to ``bacteria''.
The quotations around the topics are to make explicit that the labels
are human interpretations of what these topics may be. Topic modeling
is an unsupervised problem, and is often trained on unlabeled data.
Automatic labeling of these topics is an active area of research
\cite{mei2007automatic}, \cite{lau2011automatic}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{topic_models.png}
  \caption{\label{fig:topic_model_document} According to the trained
    topic model, topics like ``Medicine'' and ``Neural Networks''
    generate the colored words of the document}
\end{figure}

Topic modeling may discover that document in
~\autoref{fig:topic_model_document} relates to ``Medicine'' and
``Neural Networks'', because these topics give rise to the colored
words in the document.

In addition, we can view these abstract topics as a form of clustering.
Researchers are better able to navigate the large corpora of
scientific knowledge, by exploring documents that have similar topic
proportions.

In this paper, we briefly discuss the predominant model: Latent
Dirichlet Allocation (LDA) and its variants. In addition, we explore
other models that are not derivatives of LDA, such as the Replicated
Softmax.

\section{LDA}
Latent Dirichlet analysis is widely considered to be the simplest
topic model. LDA models each document as a mixture of topics, where a
topic $\beta_k$ is a probability distribution over a fixed vocabulary
of terms. Training LDA requires fixing the number of topics, $K$.
We can draw the graphical model for LDA as in
~\autoref{fig:lda_plate}.

At risk of being pedantic, we explain the graphical model in detail.
$\eta$ is the topic hyperparameter, which produces a topic
distribution $\beta_k$ of the Dirichlet family. There are a total of
$K$ topics. Similarly, $\alpha$ is a hyperparameter that produces the
per-document topic proportions $\theta_d$. These Dirichlet
distributions are of dimension $K-1$, because there are a total of $K$
topics. There are $D$ such topic proportions, where $D$ is the total
number of documents. $Z_{d,n}$ is the per-word topic assignment, drawn
from the particular $\theta_d$. Finally, $W_{d,n}$ is the $n$th word
in the $d$th document, an observed variable. It is simple to see that
$P(W_{d,n} | Z_{d,n}, \beta_{k}) = \beta_{z_{d,n}, w_{d,n}}$.

A high $\alpha$ value encodes the belief that documents contain a
mixture of many topics, rather than being largely represented by a few
topics. Similarly, a high $\eta$ value encodes the belief that topics
has high probability for a large number of words in the vocabulary.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \node[latent] (a) {$\alpha$};
    \node[latent, right=of a] (t) {$\theta_d$};
    \node[latent, right=of t] (z) {$Z_{d,n}$};
    \node[obs, right=of z] (w) {$W_{d,n}$};
    \node[latent, above=1.5cm of w] (b) {$\beta_k$};
    \node[latent, left=of b] (e) {$\eta$};

    % Edges
    \edge {a} {t};
    \edge {t} {z};
    \edge {z} {w};
    \edge {e} {b};
    \edge {b} {w};

    % Plates
    \plate [inner sep=0.3cm] {pN} {(z)(w)} {$N$};
    \plate [inner sep=0.3cm] {pM} {(t)(z)(w)(pN)} {$D$};
    \plate [inner sep=0.3cm] {pK} {(b)} {$K$};
  \end{tikzpicture}
  \caption{\label{fig:lda_plate} Plate notation for LDA, a directed
    graphical model.}
\end{figure}

We can fully specify the model, by looking at the joint distribution
$\mathcal{J}$ of all the latent and observed variables.

\begin{multline}
 \mathcal{J} = \left( \sum_{k=1}^{K} P(\beta_k | \eta) \right) \left(
    \sum_{d=1}^{D} P(\theta_d | \alpha) \right) \\
  \left( \sum_{n=1}^{N} \theta_{d, Z_{d,n}} \beta_{Z_{d,n}, W_{d,n}} \right)  
\end{multline}

The generative process of a document is as follows:

\begin{algorithm}
  \caption{Generative Process of LDA}\label{alg:DTM}
  \begin{algorithmic}[1]
    \For {each document $d$ in $D$}
    \State {Draw topic distribution $\theta_d \sim Dir(\alpha)$}
    \For {each word $n_d$ in $N_d$}
    \State {Sample topic $z_{d,n} \sim Multinomial(\theta)$}
    \State {Sample word $w_{d,n} \sim Multinomial(\beta_{z_{d,n}})$}
    \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}

If we fix the topic distributions $\beta_{1:K}$, we can compute the
per-document posterior $\theta$ given the document.

\begin{multline}
  P(\theta | w_{1:n}, \alpha, \beta_{1:K}) = \\ \frac{P(\theta |
    \alpha)\prod_{n=1}^{N} P(z_n | \theta) P(w_n | z_n,
    \beta_{1:K})}{\int_\theta P(\theta | \alpha) \prod_{n=1}^N
    \sum_{z=1}^K P(z_n | \theta) P(w_n | z_n, \beta_{1:K})}
\end{multline}


The denominator is intractable to compute, due to the coupling
between $\theta$ and $\beta$ under the multinomial assumption.
\cite{blei2003latent}. Hence, we rely on techniques for approximate
inference of the posterior. We discuss these techniques in
~\autoref{sec:inference}.

Why does LDA work? The Dirichlet distribution encourages sparsity,
encoding the belief that the document-topic distribution has few
topics per document, and the topic-word distribution has few words per
topic. These two beliefs work against each other, and LDA discovers
this sparsity balance, which gives rise to the structure of the
textual data.

\subsection{Statistical Assumptions}
\label{subsec:statistical-assumptions}
As Mackay quips, we cannot make inference without statistical
assumptions. LDA makes several assumptions, some rendering it less
suited for application to the domain of scientific documents.

LDA models documents as ``bag-of-words'': words within the document
are interchangeable. \cite{blei2003latent} I think this is a
reasonable assumption to make, given the task is to discover themes
within the document.

LDA also assumes that the order of documents do not matter.
Exchangeability of both words and documents allows LDA to model the
joint distribution as a mixture model. I believe this assumption to be
invalid in the domain of scientific documents. The meaning of
keyphrases used in scientific literature change over time. For
example, the landscape of research neural networks is vastly different
now, as compared to the 1990s, and LDA will fail to capture these
differences.

Variants of LDA relax these statistical assumptions, or make other
assumptions in place. We discuss Dynamic Topic Modeling (DTM) in
~\autoref{sec:dtm}, and briefly mention the rest in the appendix.

\section{Dynamic Topic Modeling}
\label{sec:dtm}
Dynamic Topic Modeling (DTM) was proposed to remove the assumption
that documents are \textit{exchangeable}. \cite{blei2006dynamic}

The order of documents are important for scientific documents, since
both the content, and the meaning of words evolve over time.

In DTM, data is divided by discrete time slices. The topics associated
with time slice $t$ evolve from the time slice $t-1$. Because the
Dirichlet distribution is not amenable to sequential modeling, we use
the Gaussian distribution to model the sequence of random variables.

The generative process for time slice $t$ is as follows:

\begin{algorithm}
  \caption{Generative Process of DTM}\label{alg:DTM}
  \begin{algorithmic}[1]
    \State {Draw topic distribution $\beta_t | \beta_{t-1} \sim N(\beta_{t-1},
  \sigma^2I)$}
    \State {Draw $\alpha_t | \alpha_{t-1} \sim N\left( \alpha_{t-1}, \delta^2I
    \right)$}
    \For {each document $w$}
    \State {Draw $\eta_{w,t} \sim N\left( \alpha_t, a^2I \right)$}
    \For {each word at position $n$}
    \State {Sample topic $z_{t,n} \sim Multinomial(\pi(\eta_{w,t}))$}
    \State {Sample word $w_{t,d,n} \sim Multinomial(\beta_{t,z,n})$}
    \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}

$\pi$ maps the multinomial parameters to the mean parameters,
$\pi\left( \beta_{k,t} \right)_w = \frac{exp(\beta_{k,t,w})}{\sum_w exp\left( \beta_{k,t,w} \right)}$

The Multinomial and Guassian distributions are not conjugates,
inference via Gibbs sampling is difficult. Hence, variational
inference instead.

Further extensions of this approach include the continuous Dynamic
Topic Models (cDTM), which removes the discretization of the time
slices. \cite{wang-2012-contin-time} This model has been used to
predict the timestamp of documents.

\section{Inference Methods}
\label{sec:inference}
Approximating intractable probability densities is a well-studied
problem in modern statistics. This problem arises often in Bayesian
statistics, where computing posterior probablity densities in requires
inference over latent variables. Many learning algorithms have been
developed, including collapsed Gibbs Sampling, Variational Inference,
Collapsed Variational Inference, and MAP estimation. Each of these
approximation techniques have their own strength and shortcomings.

The two inference methods are briefly discussed below, and a
comparison between them relegated to the appendix in
~\autoref{sub:choosing-inference}.

\subsection{MCMC Sampling}
\label{subsec:mcmc-sampling}
Historically, Markov Chain Monte Carlo (MCMC) sampling has been the
dominant technique for approximating posterior densities. In MCMC, we
construct an ergodic Markov chain on the latent variable $z$,
whose stationary distribution is the posterior $P( z | x)$.
Samples are drawn from the stationary distribution, and used to
approximate the posterior empirically.

In Gibbs sampling, the space of the Markov Chain is the space of the
configurations of the hidden variables. In Gibbs sampling, the next
state is reached by sequentially sampling all variables from the
distribution, conditioned on all the current sampled values. After a
``burn-in'' period, the samples would be drawn from the posterior
distribution.

\begin{algorithm}
\caption{Gibbs Sampling}\label{alg:gibbs}
\begin{algorithmic}[1]
  \State $x^{0}$ $\gets$ $q(x)$
  \For {$i = 1, 2, 3, \hdots$}
  \For {$d = 1, 2, 3, \hdots, D$}
  \State {$x_d^{i} \sim P(X_1 = x_1 | {X_k} = x_k^{i-1} \text{ for } k
    = \{1..n \setminus d\})$}  
  \EndFor
  \EndFor
\end{algorithmic}
\end{algorithm}

A full treatment of Gibbs Sampling applied to LDA can be found in
\cite{griffiths2002gibbs}.

\subsection{Variational Inference}
\label{subsec:vi}
Mean field variational inference (MFVI) breaks the coupling between
$\theta$ and $z$ by introducing free variational parameters $\gamma$
over $\theta$ and $\phi$ over $z$ and dropping the edges between them.
This results in an approximate posterior $q(\theta, z | \gamma, \phi)
= q_\gamma(\theta)\prod_nq_\phi(z_n)$.

To best approximate the true posterior, we frame it as an optimization
problem, minimizing $L$ where:

\begin{equation}
L(\gamma, \phi | \alpha, \beta) = D_{KL}\left[ q(\theta, z | \gamma,
  \phi) || p(\theta, z | \alpha, \beta) \right] - \log p(w | \alpha, \beta)
\end{equation}

This optimization has closed form coordinate descent equations for
LDA, because the Dirichlet is conjugate to the Multinomial
distribution. This computational convenience comes at the expense of
robustness, making it difficult to apply to other more complicated
topic models.

\subsection{Choosing an Inference Method}
\label{sub:choosing-inference}
How do we know which technique to use to approximate the posterior
density? MCMC methods are computationally more intensive, but provide
samples that are approximately exact from the target posterior
density. In contrast, VI methods view the problem as an optimization
problem, which allows it to utilize efficient learning algorithms such
as stochastic optimization. This is much quicker to compute, and is
suited for larger datasets.

MCMC methods, however, cover a large family of sampling methods.
Gibbs sampling requires that the prior and posterior are conjugate
distributions. When this is not possible, such as in DTM, VI methods
can perform better than other methods in the MCMC family.

A closer look at the different inference approximation algorithms,
however, shows that the performance differences can be explained away
by setting certain smoothing hyperparameters
\cite{asuncion-2012-smoot-infer}.

\section{Appendix}

\subsection{Alternatives to LDA}

\bibliography{survey}
\end{document}
